# -*- coding: utf-8 -*-
"""1910040_Tasnia_CSE425_final_assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SWdTB862-2fbqvpReEUyoABa0krGgONi
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

data = pd.read_csv("data.csv")
X = data[['Volume', 'Weight']]
y = data['CO2']

print(X)

volume = data['Volume']
weight = data['Weight']
print(weight)
print(volume)

#2a...normalizing weight
min_weight= weight.min()
#print(min_weight)
max_weight= weight.max()
#print(max_weight)
for i in range(0,36):
  weight[i]= (weight[i]-min_weight)/(max_weight-min_weight) 
print(weight)

#2a...normalizing volume
min_vol= volume.min()
print(min_vol)
max_vol= volume.max()
print(max_vol)
for i in range(0,36):
  volume[i]= (volume[i]-min_vol)/(max_vol-min_vol) 
print(volume)

X = pd.merge(volume,weight, suffixes=['Volume','Weight'],left_index= True, right_index=True)
# print(X)
X = X.to_numpy()
# print(X)

# slicing the data into train and test sets
X_train = X[:30]
#print('xtrain',X_train)

y_train = y[:30]
#print('ytrain', y_train)

X_test = X[30:]
#print('Xtest', X_test)

y_test = y[30:]

#print(y_test)

class LinearRegression:
    def __init__(self, learning_rate=0.01):
        self.learning_rate = learning_rate
    
    def fit(self, X, y, epochs=100, batch_size=10):
        n, d = X.shape
        self.W = np.random.randn(d)
        self.b = 0
        self.losses = []
        for epoch in range(epochs):
            
            index = np.random.permutation(n) #shuffle
            X = X[index]
            y = y[index]
            # Split the data into batches
            for i in range(0, n, batch_size):
                X_batch = X[i:i+batch_size]
                y_batch = y[i:i+batch_size]
                #gradient of the loss(del J) based on W and b
                y_pred = X_batch.dot(self.W) + self.b
                delta = y_pred - y_batch
                dW = X_batch.T.dot(delta) / batch_size
                db = delta.mean(axis=0)
                
                self.W -= self.learning_rate * dW
                self.b -= self.learning_rate * db

            # Compute the loss
            y_pred = X.dot(self.W) + self.b
            loss = ((y_pred - y)**2).mean() / 2
            self.losses.append(loss)
    
    def predict(self, X):
        print("Predicted CO2 emissions",X.dot(self.W) + self.b)
        return X.dot(self.W) + self.b


#Stochastic Gradient Descent

sgd = LinearRegression(learning_rate=0.1)
sgd.fit(X_train, y_train, epochs=100, batch_size=1)
y_pred_sgd = sgd.predict(X_test)


mse_sgd = ((y_pred_sgd - y_test)**2).mean()
print("MSE for Stochastic Gradient Descent is",mse_sgd)

plt.plot(sgd.losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.show()

#B
data2 = pd.read_csv("data.csv")
X2 = data2[['Type','Volume', 'Weight']]
y2 = data2['CO2']
volume2 = data2['Volume']
weight2 = data2['Weight']
typedata =data2['Type'] 

# print(weight)
# print(volume)
#print(typedata)

data2['Type'] = data2['Type'].replace([' A ', ' B ', ' C '],['00','01','10']) #converting categorical value
print(data2)

min_weight2= weight2.min()
#print(min_weight)
max_weight2= weight2.max()
#print(max_weight)
for i in range(0,36):
  weight2[i]= (weight2[i]-min_weight2)/(max_weight2-min_weight2) 
# print(weight)
min_vol2= volume2.min()
# print(min_vol)
max_vol2= volume2.max()
# print(max_vol)
for i in range(0,36):
  volume2[i]= (volume2[i]-min_vol2)/(max_vol2-min_vol2) 
# print(volume)
X2 = pd.merge(volume2,weight2, suffixes=['Volume','Weight'],left_index= True, right_index=True)
# print(X)
X2 = X2.to_numpy()
# print(X)

# Spliting data
X2_train = X[:30]
y2_train = y[:30]
X2_test = X[30:]
y2_test = y[30:]

class LinearRegression:
    def __init__(self, learning_rate=0.01):
        self.learning_rate = learning_rate
    
    def fit(self, X2, y, epochs=100, batch_size=10):
        n, d = X2.shape
        self.W = np.random.randn(d)
        self.b = 0
        self.losses = []
        for epoch in range(epochs):
           
            idx = np.random.permutation(n)
            X2 = X2[idx]
            y2 = y[idx]
            # Split the data 
            for i in range(0, n, batch_size):
                X2_batch = X2[i:i+batch_size]
                y2_batch = y2[i:i+batch_size]
               
                y2_pred = X2_batch.dot(self.W) + self.b
                delta = y2_pred - y2_batch
                dW = X2_batch.T.dot(delta) / batch_size
                db = delta.mean(axis=0)
               
                self.W -= self.learning_rate * dW
                self.b -= self.learning_rate * db
            # Compute the loss
            y2_pred = X2.dot(self.W) + self.b
            loss = ((y2_pred - y2)**2).mean() / 2
            self.losses.append(loss)
    
    def predict(self, X2):
        return X2.dot(self.W) + self.b


bgd = LinearRegression(learning_rate=0.1)
bgd.fit(X2_train, y2_train, epochs=100, batch_size=len(X_train))
y2_pred_bgd = bgd.predict(X2_test)
mse_bgd = ((y2_pred_bgd - y2_test)**2).mean()
print("MSE for Batch Gradient Descent is",mse_bgd)
plt.plot(bgd.losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.show()

#Stochastic Gradient Descent

sgd = LinearRegression(learning_rate=0.1)
sgd.fit(X2_train, y2_train, epochs=100, batch_size=1)
y2_pred_sgd = sgd.predict(X2_test)


mse_sgd = ((y2_pred_sgd - y2_test)**2).mean()
print("MSE for Stochastic Gradient Descent is",mse_sgd)
plt.plot(sgd.losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.show()

